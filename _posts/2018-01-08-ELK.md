---  
layout: post  
title: ELK  
date: 2018-01-08  
categories: blog  
tags: [Java]  
description: ELK log analysis  
  
---  

========== ELK 日志分析 =======

-- 原文 http://baidu.blog.51cto.com/71938/1676798

	日志主要包括系统日志、应用程序日志和安全日志。系统运维和开发人员可以通过日志了解服务器软硬件信息、检查配置过程中的错误及错误发生的原因。经常分析日志可以了解服务器的负荷，性能安全性，从而及时采取措施纠正错误。
	通常，日志被分散的储存不同的设备上。如果你管理数十上百台服务器，你还在使用依次登录每台机器的传统方法查阅日志。这样是不是感觉很繁琐和效率低下。当务之急我们使用集中化的日志管理，例如：开源的syslog，将所有服务器上的日志收集汇总。
	集中化管理日志后，日志的统计和检索又成为一件比较麻烦的事情，一般我们使用grep、awk和wc等Linux命令能实现检索和统计，但是对于要求更高的查询、排序和统计等要求和庞大的机器数量依然使用这样的方法难免有点力不从心。
	开源实时日志分析ELK平台能够完美的解决我们上述的问题，ELK由ElasticSearch、Logstash和Kiabana三个开源工具组成。官方网站：https://www.elastic.co/products
	-  Elasticsearch是个开源分布式搜索引擎，它的特点有： 分布式，零配置，自动发现，索引自动分片，索引副本机制，restful风格接口，多数据源，自动搜索负载等。
	-  Logstash是一个完全开源的工具，他可以对你的日志进行收集、分析，并将其存储供以后使用（如，搜索）。
	-  kibana 也是一个开源和免费的工具，他Kibana可以为 Logstash 和 ElasticSearch 提供的日志分析友好的 Web 界面，可以帮助您汇总、分析和搜索重要数据日志。
     
搭建过程：
1. Application Server : 
	1. 安装jdk
	2. 安装Logstash : 解压,在安装目录添加logstash-simple.conf
			input {
				 beats {
						 type => beats
						 port => 5051
				 }
			}
			filter {
				grok {
					match => [ "message", "^CSI_ID:%{NUMBER:CSIID}; ROW_ID:%{GREEDYDATA:rowId}; APP_MANAGER:%{GREEDYDATA:appManager}; PACKAGE_NAME:%{GREEDYDATA:packageName}; FILE_NAME:%{GREEDYDATA:fileName}; RANK_SCORE:%{NUMBER:rank_score}; LINE_NUMBER:%{NUMBER:lineNumber}; CONTENT:%{GREEDYDATA:content}"]
				}
				mutate {
				  convert => [ "CSIID", "integer"]
				  convert => [ "rank_score", "integer"]
				  convert => [ "lineNumber", "integer"]
				}
			}
			output {
				elasticsearch {
									hosts => ["localhost"]
									index => "detectpwd-test-%{+YYYY.MM.dd}"
									document_id =>"%{rowId}"
				}
				stdout { codec => rubydebug }
			}
		-----------------file-------------------------------	
		# create index for local file
			input {
			  file {
				type =>"syslog"
				 path => ["/var/log/messages", "/var/log/syslog" ]
			  }
			  syslog {
				type =>"syslog"
				port =>"5544"
			  }
			}
			output {
			  stdout { codec=> rubydebug }
			  elasticsearch {host => "localhost" }
			}	
	3. 启动Loggstatsh
		 /usr/local/logstash-1.5.2/bin/logstash agent -f logstash-simple.conf
	4. 安装Elasticsearch ：　解压 
	5. 启动Elasticsearch
	6. 安装Elasticsearch-kopf插件，可以查询Elasticsearch中的数据：
		# cd /usr/local/elasticsearch-1.6.0/
		# ./plugin -install lmenezes/elasticsearch-kopf
		# ls plugins/
			kopf
		浏览器访问 ：　http://localhost:9200/_plugin/kopf
	7. 安装Kibana　：　解压 
	8. 启动Kibana : bin/kibana
		http://localhost：5601
	9. Kibana :
		1. Management -> Index patterns -> + Add New -> IndexName(same with index in logstash output)
		2. Set as default Index
		3. discover -> select time as "today"/Others (will display the data with default index); Save it with name = "discover test"
		4. dashboard -> add "discover test" -> save it "dashboard test"
		5. create Visualize and and save it : https://www.elastic.co/guide/en/kibana/5.0/index.html
		6. add it to dashboard and save dashboard
		
